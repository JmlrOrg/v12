{
    "abstract": "Co-training (or more generally, co-regularization) has been a popular algorithm for semi-supervised learning in data with two feature representations (or views), but the fundamental assumptions underlying this type of models are still unclear. In this paper we propose a Bayesian undirected graphical model for co-training, or more generally for semi-supervised multi-view learning. This makes explicit the previously unstated assumptions of a large class of co-training type algorithms, and also clarifies the circumstances under which these assumptions fail. Building upon new insights from this model, we propose an improved method for co-training, which is a novel co-training kernel for Gaussian process classifiers. The resulting approach is convex and avoids local-maxima problems, and it can also automatically estimate how much each view should be trusted to accommodate noisy or unreliable views. The Bayesian co-training approach can also elegantly handle data samples with missing views, that is, some of the views are not available for some data points at learning time. This is further extended to an active sensing framework, in which the missing (sample, view) pairs are actively acquired to improve learning performance. The strength of active sensing model is that one actively sensed (sample, view) pair would improve the joint multi-view classification on all the samples.  Experiments on toy data and several real world data sets illustrate the benefits of this approach.",
    "authors": [
        "Shipeng Yu",
        "Balaji Krishnapuram",
        "R&#243;mer Rosales",
        "R. Bharat Rao"
    ],
    "id": "yu11a",
    "issue": 79,
    "pages": [
        2649,
        2680
    ],
    "title": "Bayesian Co-Training",
    "volume": "12",
    "year": "2011"
}