{
    "abstract": "The problem of learning forest-structured discrete graphical models from i.i.d. samples is considered. An  algorithm based on pruning of the Chow-Liu tree through adaptive thresholding is proposed.   It is shown that this algorithm is both  structurally consistent and risk consistent and  the error probability of structure learning decays faster than any polynomial in the number of samples   under fixed model size.  For the  high-dimensional scenario where the size of the  model <i>d</i> and the number of edges <i>k</i> scale with the number of samples <i>n</i>,  sufficient conditions on <i>(n,d,k)</i> are given for  the algorithm to satisfy structural and risk consistencies. In addition, the extremal structures for learning are identified; we prove that the independent (resp., tree) model is the hardest (resp., easiest) to learn using the proposed algorithm in terms of error rates for structure learning.",
    "authors": [
        "Vincent Y.F. Tan",
        "Animashree Anandkumar",
        "Alan S. Willsky"
    ],
    "id": "tan11a",
    "issue": 44,
    "pages": [
        1617,
        1653
    ],
    "title": "Learning High-Dimensional Markov Forest Distributions: Analysis of Error Rates",
    "volume": "12",
    "year": "2011"
}